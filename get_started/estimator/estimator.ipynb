{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Construction ... Beware ... A Work In Progress...\n",
    "A lot has changed in this area and it took some time to get this example to work again but it will work. Stay tuned for further updates to clean this up.\n",
    "\n",
    "# tf.estimator Quickstart\n",
    "\n",
    "TensorFlow's high-level machine learning API (tf.estimator) makes it easy to\n",
    "configure, train, and evaluate a variety of machine learning models. In this\n",
    "tutorial, you'll use tf.estimator to construct a\n",
    "[neural network](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "classifier and train it on the\n",
    "[Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) to\n",
    "predict flower species based on sepal/petal geometry. You'll write code to\n",
    "perform the following five steps:\n",
    "\n",
    "1.  Load CSVs containing Iris training/test data into a TensorFlow `Dataset`\n",
    "2.  Construct a @{tf.estimator.DNNClassifier$neural network classifier}\n",
    "3.  Train the model using the training data\n",
    "4.  Evaluate the accuracy of the model\n",
    "5.  Classify new samples\n",
    "\n",
    "NOTE: Remember to @{$install$install TensorFlow on your machine}\n",
    "before getting started with this tutorial.\n",
    "\n",
    "## Complete Neural Network Source Code\n",
    "\n",
    "Here is the full code for the neural network classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def main():\n",
    "  # Load datasets.\n",
    "  if not os.path.exists(IRIS_TRAINING):\n",
    "      raw = urllib.request.urlopen(IRIS_TRAINING_URL).read().decode()\n",
    "      with open(IRIS_TRAINING,'w') as f:\n",
    "        f.write(raw)\n",
    "\n",
    "  if not os.path.exists(IRIS_TEST):\n",
    "      raw = urllib.request.urlopen(IRIS_TEST_URL).read().decode()\n",
    "      with open(IRIS_TEST,'w') as f:\n",
    "        f.write(raw)\n",
    "\n",
    "    # Load datasets.\n",
    "  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "  # Specify that all features have real-value data\n",
    "  feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "  classifier = tf.contrib.learn.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=[10, 20, 10],\n",
    "      n_classes=3,\n",
    "      model_dir=\"/tmp/iris_model\",\n",
    "      config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\n",
    "\n",
    "  classifier.fit(x=training_set.data,\n",
    "                 y=training_set.target,\n",
    "                 steps=2000)\n",
    "\n",
    "\n",
    "  # Evaluate accuracy.\n",
    "  accuracy_score = classifier.evaluate(\n",
    "      x=test_set.data, y=test_set.target)[\"accuracy\"]\n",
    "  print(\"Accuracy: {0:f}\".format(accuracy_score))\n",
    "\n",
    "  # Classify three new samples\n",
    "  new_samples = np.array(\n",
    "      [[7.0, 3.1, 4.9, 1.5], [4.8, 3.2, 1.3, 0.2],  [6.0, 3.0, 5.1, 1.8]], dtype=np.float32)\n",
    "  y = list(classifier.predict(new_samples))\n",
    "  print(\"Predictions: {}\".format(str(y)))\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections walk through the code in detail.\n",
    "\n",
    "## Load the Iris CSV data to TensorFlow\n",
    "\n",
    "The [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) contains\n",
    "150 rows of data, comprising 50 samples from each of three related Iris species:\n",
    "*Iris setosa*, *Iris virginica*, and *Iris versicolor*.\n",
    "\n",
    "![Petal geometry compared for three iris species: Iris setosa, Iris virginica, and Iris versicolor](https://www.tensorflow.org/images/iris_three_species.jpg) **From left to right,\n",
    "[*Iris setosa*](https://commons.wikimedia.org/w/index.php?curid=170298) (by\n",
    "[Radomil](https://commons.wikimedia.org/wiki/User:Radomil), CC BY-SA 3.0),\n",
    "[*Iris versicolor*](https://commons.wikimedia.org/w/index.php?curid=248095) (by\n",
    "[Dlanglois](https://commons.wikimedia.org/wiki/User:Dlanglois), CC BY-SA 3.0),\n",
    "and [*Iris virginica*](https://www.flickr.com/photos/33397993@N05/3352169862)\n",
    "(by [Frank Mayfield](https://www.flickr.com/photos/33397993@N05), CC BY-SA\n",
    "2.0).**\n",
    "\n",
    "Each row contains the following data for each flower sample:\n",
    "[sepal](https://en.wikipedia.org/wiki/Sepal) length, sepal width,\n",
    "[petal](https://en.wikipedia.org/wiki/Petal) length, petal width, and flower\n",
    "species. Flower species are represented as integers, with 0 denoting *Iris\n",
    "setosa*, 1 denoting *Iris versicolor*, and 2 denoting *Iris virginica*.\n",
    "\n",
    "Sepal Length | Sepal Width | Petal Length | Petal Width | Species\n",
    ":----------- | :---------- | :----------- | :---------- | :-------\n",
    "5.1          | 3.5         | 1.4          | 0.2         | 0\n",
    "4.9          | 3.0         | 1.4          | 0.2         | 0\n",
    "4.7          | 3.2         | 1.3          | 0.2         | 0\n",
    "&hellip;     | &hellip;    | &hellip;     | &hellip;    | &hellip;\n",
    "7.0          | 3.2         | 4.7          | 1.4         | 1\n",
    "6.4          | 3.2         | 4.5          | 1.5         | 1\n",
    "6.9          | 3.1         | 4.9          | 1.5         | 1\n",
    "&hellip;     | &hellip;    | &hellip;     | &hellip;    | &hellip;\n",
    "6.5          | 3.0         | 5.2          | 2.0         | 2\n",
    "6.2          | 3.4         | 5.4          | 2.3         | 2\n",
    "5.9          | 3.0         | 5.1          | 1.8         | 2\n",
    "\n",
    "For this tutorial, the Iris data has been randomized and split into two separate\n",
    "CSVs:\n",
    "\n",
    "*   A training set of 120 samples\n",
    "    ([iris_training.csv](http://download.tensorflow.org/data/iris_training.csv))\n",
    "*   A test set of 30 samples\n",
    "    ([iris_test.csv](http://download.tensorflow.org/data/iris_test.csv)).\n",
    "\n",
    "To get started, first import all the necessary modules, and define where to\n",
    "download and store the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if the training and test sets aren't already stored locally, download\n",
    "them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(IRIS_TRAINING):\n",
    "  raw = urllib.request.urlopen(IRIS_TRAINING_URL).read().decode()\n",
    "  with open(IRIS_TRAINING,'w') as f:\n",
    "    f.write(raw)\n",
    "\n",
    "if not os.path.exists(IRIS_TEST):\n",
    "  raw = urllib.request.urlopen(IRIS_TEST_URL).read().decode()\n",
    "  with open(IRIS_TEST,'w') as f:\n",
    "    f.write(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the training and test sets into `Dataset`s using the\n",
    "[`load_csv_with_header()`](https://www.tensorflow.org/code/tensorflow/contrib/learn/python/learn/datasets/base.py)\n",
    "method in `learn.datasets.base`. The `load_csv_with_header()` method takes three\n",
    "required arguments:\n",
    "\n",
    "*   `filename`, which takes the filepath to the CSV file\n",
    "*   `target_dtype`, which takes the\n",
    "    [`numpy` datatype](http://docs.scipy.org/doc/numpy/user/basics.types.html)\n",
    "    of the dataset's target value.\n",
    "*   `features_dtype`,which takes the\n",
    "    [`numpy` datatype](http://docs.scipy.org/doc/numpy/user/basics.types.html)\n",
    "    of the dataset's feature values.\n",
    "\n",
    "\n",
    "Here, the target (the value you're training the model to predict) is flower\n",
    "species, which is an integer from 0&ndash;2, so the appropriate `numpy` datatype\n",
    "is `np.int`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset`s in tf.contrib.learn are\n",
    "[named tuples](https://docs.python.org/2/library/collections.html#collections.namedtuple);\n",
    "you can access feature data and target values via the `data` and `target`\n",
    "fields. Here, `training_set.data` and `training_set.target` contain the feature\n",
    "data and target values for the training set, respectively, and `test_set.data`\n",
    "and `test_set.target` contain feature data and target values for the test set.\n",
    "\n",
    "Later on, in\n",
    "[\"Fit the DNNClassifier to the Iris Training Data,\"](#fit-dnnclassifier)\n",
    "you'll use `training_set.data` and\n",
    "`training_set.target` to train your model, and in\n",
    "[\"Evaluate Model Accuracy,\"](#evaluate-accuracy) you'll use `test_set.data` and\n",
    "`test_set.target`. But first, you'll construct your model in the next section.\n",
    "\n",
    "## Construct a Deep Neural Network Classifier\n",
    "\n",
    "tf.estimator offers a variety of predefined models, called `Estimator`s, which\n",
    "you can use \"out of the box\" to run training and evaluation operations on your\n",
    "data.\n",
    "Here, you'll configure a Deep Neural Network Classifier model to fit the Iris\n",
    "data. Using tf.estimator, you can instantiate your\n",
    "@{tf.estimator.DNNClassifier} with just a couple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=[10, 20, 10],\n",
    "      n_classes=3,\n",
    "      model_dir=\"/tmp/iris_model\",\n",
    "      config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above first defines the model's feature columns, which specify the data\n",
    "type for the features in the data set. All the feature data is continuous, so\n",
    "`tf.feature_column.numeric_column` is the appropriate function to use to\n",
    "construct the feature columns. There are four features in the data set (sepal\n",
    "width, sepal height, petal width, and petal height), so accordingly `shape`\n",
    "must be set to `[4]` to hold all the data.\n",
    "\n",
    "Then, the code creates a `DNNClassifier` model using the following arguments:\n",
    "\n",
    "*   `feature_columns=feature_columns`. The set of feature columns defined above.\n",
    "*   `hidden_units=[10, 20, 10]`. Three\n",
    "    [hidden layers](http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw),\n",
    "    containing 10, 20, and 10 neurons, respectively.\n",
    "*   `n_classes=3`. Three target classes, representing the three Iris species.\n",
    "*   `model_dir=/tmp/iris_model`. The directory in which TensorFlow will save\n",
    "    checkpoint data during model training. For more on logging and monitoring\n",
    "    with TensorFlow, see\n",
    "    @{$monitors$Logging and Monitoring Basics with tf.estimator}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the DNNClassifier to the Iris Training Data {#fit-dnnclassifier}\n",
    "\n",
    "Now that you've configured your DNN `classifier` model, you can fit it to the\n",
    "Iris training data using the @{tf.estimator.Estimator.train$`train`} method.\n",
    "Pass `train_input_fn` as the `input_fn`, and the number of steps to train\n",
    "(here, 2000):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x=training_set.data,\n",
    "                 y=training_set.target,\n",
    "                 steps=2000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of the model is preserved in the `classifier`, which means you can\n",
    "train iteratively if you like. For example, the above is equivalent to the\n",
    "following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you're looking to track the model while it trains, you'll likely\n",
    "want to instead use a TensorFlow @{tf.train.SessionRunHook$`SessionRunHook`}\n",
    "to perform logging operations. See the tutorial\n",
    "@{$monitors$Logging and Monitoring Basics with tf.estimator}\n",
    "for more on this topic.\n",
    "\n",
    "## Evaluate Model Accuracy {#evaluate-accuracy}\n",
    "\n",
    "You've trained your `DNNClassifier` model on the Iris training data; now, you\n",
    "can check its accuracy on the Iris test data using the\n",
    "@{tf.estimator.Estimator.evaluate$`evaluate`} method. Like `train`,\n",
    "`evaluate` takes an input function that builds its input pipeline. `evaluate`\n",
    "returns a `dict`s with the evaluation results. The following code passes the\n",
    "Iris test data&mdash;`test_set.data` and `test_set.target`&mdash;to `evaluate`\n",
    "and prints the `accuracy` from the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(\n",
    "      x=test_set.data, y=test_set.target)[\"accuracy\"]\n",
    "print(\"Accuracy: {0:f}\".format(accuracy_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Classify three new samples\n",
    "  new_samples = np.array(\n",
    "      [[7.0, 3.1, 4.9, 1.5], [4.8, 3.2, 1.3, 0.2],  [6.0, 3.0, 5.1, 1.8]], dtype=np.float32)\n",
    "  y = list(classifier.predict(new_samples))\n",
    "  print(\"Predictions: {}\".format(str(y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
