{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# In Progress. Not ready for use.\n",
    "\n",
    "# TensorFlow Mechanics 101\n",
    "\n",
    "Code: [tensorflow/examples/tutorials/mnist/](https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/)\n",
    "\n",
    "The goal of this tutorial is to show how to use TensorFlow to train and\n",
    "evaluate a simple feed-forward neural network for handwritten digit\n",
    "classification using the (classic) MNIST data set.  The intended audience for\n",
    "this tutorial is experienced machine learning users interested in using\n",
    "TensorFlow.\n",
    "\n",
    "These tutorials are not intended for teaching Machine Learning in general.\n",
    "\n",
    "Please ensure you have followed the instructions to\n",
    "@{$install$install TensorFlow}.\n",
    "\n",
    "## Tutorial Files\n",
    "\n",
    "This tutorial references the following files:\n",
    "\n",
    "File | Purpose\n",
    "--- | ---\n",
    "[`mnist.py`](./mnist.py) | The code to build a fully-connected MNIST model.\n",
    "[`fully_connected_feed.py`](./fully_connected_feed.py) | The main code to train the built MNIST model against the downloaded dataset using a feed dictionary.\n",
    "\n",
    "Simply run the `fully_connected_feed.py` file directly to start training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "MNIST is a classic problem in machine learning. The problem is to look at\n",
    "greyscale 28x28 pixel images of handwritten digits and determine which digit\n",
    "the image represents, for all the digits from zero to nine.\n",
    "\n",
    "![MNIST Digits](https://www.tensorflow.org/images/mnist_digits.png \"MNIST Digits\")\n",
    "\n",
    "For more information, refer to [Yann LeCun's MNIST page](http://yann.lecun.com/exdb/mnist/)\n",
    "or [Chris Olah's visualizations of MNIST](http://colah.github.io/posts/2014-10-Visualizing-MNIST/).\n",
    "\n",
    "### Download\n",
    "\n",
    "At the top of the `run_training()` method, the `input_data.read_data_sets()`\n",
    "function will ensure that the correct data has been downloaded to your local\n",
    "training folder and then unpack that data to return a dictionary of `DataSet`\n",
    "instances.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "data_sets = input_data.read_data_sets('./train_dir', './fake_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_sets)\n",
    "print(type(data_sets.train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE**: The `fake_data` flag is used for unit-testing purposes and may be\n",
    "safely ignored by the reader.\n",
    "\n",
    "Dataset | Purpose\n",
    "--- | ---\n",
    "`data_sets.train` | 55000 images and labels, for primary training.\n",
    "`data_sets.validation` | 5000 images and labels, for iterative validation of training accuracy.\n",
    "`data_sets.test` | 10000 images and labels, for final testing of trained accuracy.\n",
    "\n",
    "### Inputs and Placeholders\n",
    "\n",
    "The `placeholder_inputs()` function creates two @{tf.placeholder}\n",
    "ops that define the shape of the inputs, including the `batch_size`, to the\n",
    "rest of the graph and into which the actual training examples will be fed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "hidden1 = 128\n",
    "hidden2 = 32\n",
    "learning_rate = 0.01\n",
    "log_dir = \"./log_dir\"\n",
    "max_steps=2000\n",
    "fake_data=False\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                       mnist.IMAGE_PIXELS))\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further down, in the training loop, the full image and label datasets are\n",
    "sliced to fit the `batch_size` for each step, matched with these placeholder\n",
    "ops, and then passed into the `sess.run()` function using the `feed_dict`\n",
    "parameter.\n",
    "\n",
    "## Build the Graph\n",
    "\n",
    "After creating placeholders for the data, the graph is built from the\n",
    "`mnist.py` file according to a 3-stage pattern: `inference()`, `loss()`, and\n",
    "`training()`.\n",
    "\n",
    "1.  `inference()` - Builds the graph as far as required for running\n",
    "the network forward to make predictions.\n",
    "1.  `loss()` - Adds to the inference graph the ops required to generate\n",
    "loss.\n",
    "1.  `training()` - Adds to the loss graph the ops required to compute\n",
    "and apply gradients.\n",
    "\n",
    "<div style=\"width:95%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
    "  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/mnist_subgraph.png\">\n",
    "</div>\n",
    "\n",
    "### Inference\n",
    "\n",
    "The `inference()` function builds the graph as far as needed to\n",
    "return the tensor that would contain the output predictions.\n",
    "\n",
    "It takes the images placeholder as input and builds on top\n",
    "of it a pair of fully connected layers with [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation followed by a ten\n",
    "node linear layer specifying the output logits.\n",
    "\n",
    "Each layer is created beneath a unique @{tf.name_scope}\n",
    "that acts as a prefix to the items created within that scope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Within the defined scope, the weights and biases to be used by each of these\n",
    "layers are generated into @{tf.Variable}\n",
    "instances, with their desired shapes:\n",
    "\n",
    "When, for instance, these are created under the `hidden1` scope, the unique\n",
    "name given to the weights variable would be \"`hidden1/weights`\".\n",
    "\n",
    "Each variable is given initializer ops as part of their construction.\n",
    "\n",
    "In this most common case, the weights are initialized with the\n",
    "@{tf.truncated_normal}\n",
    "and given their shape of a 2-D tensor with\n",
    "the first dim representing the number of units in the layer from which the\n",
    "weights connect and the second dim representing the number of\n",
    "units in the layer to which the weights connect.  For the first layer, named\n",
    "`hidden1`, the dimensions are `[IMAGE_PIXELS, hidden1_units]` because the\n",
    "weights are connecting the image inputs to the hidden1 layer.  The\n",
    "`tf.truncated_normal` initializer generates a random distribution with a given\n",
    "mean and standard deviation.\n",
    "\n",
    "Then the biases are initialized with @{tf.zeros}\n",
    "to ensure they start with all zero values, and their shape is simply the number\n",
    "of units in the layer to which they connect.\n",
    "\n",
    "The graph's three primary ops -- two @{tf.nn.relu}\n",
    "ops wrapping @{tf.matmul}\n",
    "for the hidden layers and one extra `tf.matmul` for the logits -- are then\n",
    "created, each in turn, with separate `tf.Variable` instances connected to each\n",
    "of the input placeholders or the output tensors of the previous layer.\n",
    "\n",
    "Finally, the `logits` tensor that will contain the output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "The `loss()` function further builds the graph by adding the required loss\n",
    "ops.\n",
    "\n",
    "First, the values from the `labels_placeholder` are converted to 64-bit integers. Then, a @{tf.nn.sparse_softmax_cross_entropy_with_logits} op is added to automatically produce 1-hot labels from the `labels_placeholder` and compare the output logits from the `inference()` function with those 1-hot labels.\n",
    "\n",
    "It then uses @{tf.reduce_mean}\n",
    "to average the cross entropy values across the batch dimension (the first\n",
    "dimension) as the total loss.\n",
    "\n",
    "\n",
    "And the tensor that will then contain the loss value is returned.\n",
    "\n",
    "> Note: Cross-entropy is an idea from information theory that allows us\n",
    "> to describe how bad it is to believe the predictions of the neural network,\n",
    "> given what is actually true. For more information, read the blog post Visual\n",
    "> Information Theory (http://colah.github.io/posts/2015-09-Visual-Information/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits, name='xentropy')\n",
    "  return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The `training()` function adds the operations needed to minimize the loss via\n",
    "[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "Firstly, it takes the loss tensor from the `loss()` function and hands it to a\n",
    "@{tf.summary.scalar},\n",
    "an op for generating summary values into the events file when used with a\n",
    "@{tf.summary.FileWriter} (see below).  In this case, it will emit the snapshot value of\n",
    "the loss every time the summaries are written out.\n",
    "\n",
    "Next, we instantiate a @{tf.train.GradientDescentOptimizer}\n",
    "responsible for applying gradients with the requested learning rate.\n",
    "\n",
    "We then generate a single variable to contain a counter for the global\n",
    "training step and the @{tf.train.Optimizer.minimize}\n",
    "op is used to both update the trainable weights in the system and increment the\n",
    "global step.  This op is, by convention, known as the `train_op` and is what must\n",
    "be run by a TensorFlow session in order to induce one full step of training\n",
    "(see below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    " \n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Once the graph is built, it can be iteratively trained and evaluated in a loop\n",
    "controlled by the user code in `fully_connected_feed.py`.\n",
    "\n",
    "### The Graph\n",
    "\n",
    "At the top of the `run_training()` function is a python `with` command that\n",
    "indicates all of the built ops are to be associated with the default\n",
    "global @{tf.Graph}\n",
    "instance.\n",
    "\n",
    "```python\n",
    "with tf.Graph().as_default():\n",
    "```\n",
    "\n",
    "A `tf.Graph` is a collection of ops that may be executed together as a group.\n",
    "Most TensorFlow uses will only need to rely on the single default graph.\n",
    "\n",
    "More complicated uses with multiple graphs are possible, but beyond the scope of\n",
    "this simple tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mnist.inference(images_placeholder,\n",
    "                             hidden1,\n",
    "                             hidden2)\n",
    "\n",
    "# Add to the Graph the Ops for loss calculation.\n",
    "loss = mnist.loss(logits, labels_placeholder)\n",
    "\n",
    "# Add to the Graph the Ops that calculate and apply gradients.\n",
    "train_op = mnist.training(loss, learning_rate)\n",
    "\n",
    "# Add the Op to compare the logits to the labels during evaluation.\n",
    "eval_correct = mnist.evaluation(logits, labels_placeholder)\n",
    "\n",
    "# Build the summary Tensor based on the TF collection of Summaries.\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# Add the variable initializer Op.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (hidden1, hidden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Session\n",
    "\n",
    "Once all of the build preparation has been completed and all of the necessary\n",
    "ops generated, a @{tf.Session}\n",
    "is created for running the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, a `Session` may be generated into a `with` block for scoping:\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "```\n",
    "\n",
    "The empty parameter to session indicates that this code will attach to\n",
    "(or create if not yet created) the default local session.\n",
    "\n",
    "Immediately after creating the session, all of the `tf.Variable`\n",
    "instances are initialized by calling @{tf.Session.run}\n",
    "on their initialization op.\n",
    "\n",
    "```python\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "```\n",
    "\n",
    "The @{tf.Session.run}\n",
    "method will run the complete subset of the graph that\n",
    "corresponds to the op(s) passed as parameters.  In this first call, the `init`\n",
    "op is a @{tf.group}\n",
    "that contains only the initializers for the variables.  None of the rest of the\n",
    "graph is run here; that happens in the training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "\n",
    "# And then after everything is built:\n",
    "\n",
    "# Run the Op to initialize the variables.\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         mnist.IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size` examples.\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size,\n",
    "                                                 fake_data)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop\n",
    "\n",
    "After initializing the variables with the session, training may begin.\n",
    "\n",
    "The user code controls the training per step, and the simplest loop that\n",
    "can do useful training is:\n",
    "\n",
    "```python\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    sess.run(train_op)\n",
    "```\n",
    "\n",
    "However, this tutorial is slightly more complicated in that it must also slice\n",
    "up the input data for each step to match the previously generated placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print (data_sets)\n",
    "for step in iter(range(max_steps)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fill a feed dictionary with the actual set of images and labels\n",
    "    # for this particular training step.\n",
    "    feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "\n",
    "    # Run one step of the model.  The return values are the activations\n",
    "    # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "    # inspect the values of your Ops or variables, you may include them\n",
    "    # in the list passed to sess.run() and the value tensors will be\n",
    "    # returned in the tuple from the call.\n",
    "    _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    # Write the summaries and print an overview fairly often.\n",
    "    if step % 100 == 0:\n",
    "      # Print status to stdout.\n",
    "      print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "      # Update the events file.\n",
    "      summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "      summary_writer.add_summary(summary_str, step)\n",
    "      summary_writer.flush()\n",
    "\n",
    "    # Save a checkpoint and evaluate the model periodically.\n",
    "    if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "      checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "      saver.save(sess, checkpoint_file, global_step=step)\n",
    "      # Evaluate against the training set.\n",
    "      print('Training Data Eval:')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              images_placeholder,\n",
    "              labels_placeholder,\n",
    "              data_sets.train)\n",
    "      # Evaluate against the validation set.\n",
    "      print('Validation Data Eval:')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              images_placeholder,\n",
    "              labels_placeholder,\n",
    "              data_sets.validation)\n",
    "      # Evaluate against the test set.\n",
    "      print('Test Data Eval:')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              images_placeholder,\n",
    "              labels_placeholder,\n",
    "              data_sets.test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed the Graph\n",
    "\n",
    "For each step, the code will generate a feed dictionary that will contain the\n",
    "set of examples on which to train for the step, keyed by the placeholder\n",
    "ops they represent.\n",
    "\n",
    "In the `fill_feed_dict()` function, the given `DataSet` is queried for its next\n",
    "`batch_size` set of images and labels, and tensors matching the placeholders are\n",
    "filled containing the next images and labels.\n",
    "\n",
    "```python\n",
    "images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                               FLAGS.fake_data)\n",
    "```\n",
    "\n",
    "A python dictionary object is then generated with the placeholders as keys and\n",
    "the representative feed tensors as values.\n",
    "\n",
    "```python\n",
    "feed_dict = {\n",
    "    images_placeholder: images_feed,\n",
    "    labels_placeholder: labels_feed,\n",
    "}\n",
    "```\n",
    "\n",
    "This is passed into the `sess.run()` function's `feed_dict` parameter to provide\n",
    "the input examples for this step of training.\n",
    "\n",
    "#### Check the Status\n",
    "\n",
    "The code specifies two values to fetch in its run call: `[train_op, loss]`.\n",
    "\n",
    "```python\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    feed_dict = fill_feed_dict(data_sets.train,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    _, loss_value = sess.run([train_op, loss],\n",
    "                             feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "Because there are two values to fetch, `sess.run()` returns a tuple with two\n",
    "items.  Each `Tensor` in the list of values to fetch corresponds to a numpy\n",
    "array in the returned tuple, filled with the value of that tensor during this\n",
    "step of training. Since `train_op` is an `Operation` with no output value, the\n",
    "corresponding element in the returned tuple is `None` and, thus,\n",
    "discarded. However, the value of the `loss` tensor may become NaN if the model\n",
    "diverges during training, so we capture this value for logging.\n",
    "\n",
    "Assuming that the training runs fine without NaNs, the training loop also\n",
    "prints a simple status text every 100 steps to let the user know the state of\n",
    "training.\n",
    "\n",
    "```python\n",
    "if step % 100 == 0:\n",
    "    print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "```\n",
    "\n",
    "#### Visualize the Status\n",
    "\n",
    "In order to emit the events files used by @{$summaries_and_tensorboard$TensorBoard},\n",
    "all of the summaries (in this case, only one) are collected into a single Tensor\n",
    "during the graph building phase.\n",
    "\n",
    "```python\n",
    "summary = tf.summary.merge_all()\n",
    "```\n",
    "\n",
    "And then after the session is created, a @{tf.summary.FileWriter}\n",
    "may be instantiated to write the events files, which\n",
    "contain both the graph itself and the values of the summaries.\n",
    "\n",
    "```python\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "```\n",
    "\n",
    "Lastly, the events file will be updated with new summary values every time the\n",
    "`summary` is evaluated and the output passed to the writer's `add_summary()`\n",
    "function.\n",
    "\n",
    "```python\n",
    "summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "summary_writer.add_summary(summary_str, step)\n",
    "```\n",
    "\n",
    "When the events files are written, TensorBoard may be run against the training\n",
    "folder to display the values from the summaries.\n",
    "\n",
    "![MNIST TensorBoard](https://www.tensorflow.org/images/mnist_tensorboard.png \"MNIST TensorBoard\")\n",
    "\n",
    "**NOTE**: For more info about how to build and run Tensorboard, please see the accompanying tutorial @{$summaries_and_tensorboard$Tensorboard: Visualizing Learning}.\n",
    "\n",
    "#### Save a Checkpoint\n",
    "\n",
    "In order to emit a checkpoint file that may be used to later restore a model\n",
    "for further training or evaluation, we instantiate a\n",
    "@{tf.train.Saver}.\n",
    "\n",
    "```python\n",
    "saver = tf.train.Saver()\n",
    "```\n",
    "\n",
    "In the training loop, the @{tf.train.Saver.save}\n",
    "method will periodically be called to write a checkpoint file to the training\n",
    "directory with the current values of all the trainable variables.\n",
    "\n",
    "```python\n",
    "saver.save(sess, FLAGS.train_dir, global_step=step)\n",
    "```\n",
    "\n",
    "At some later point in the future, training might be resumed by using the\n",
    "@{tf.train.Saver.restore}\n",
    "method to reload the model parameters.\n",
    "\n",
    "```python\n",
    "saver.restore(sess, FLAGS.train_dir)\n",
    "```\n",
    "\n",
    "## Evaluate the Model\n",
    "\n",
    "Every thousand steps, the code will attempt to evaluate the model against both\n",
    "the training and test datasets.  The `do_eval()` function is called thrice, for\n",
    "the training, validation, and test datasets.\n",
    "\n",
    "```python\n",
    "print('Training Data Eval:')\n",
    "do_eval(sess,\n",
    "        eval_correct,\n",
    "        images_placeholder,\n",
    "        labels_placeholder,\n",
    "        data_sets.train)\n",
    "print('Validation Data Eval:')\n",
    "do_eval(sess,\n",
    "        eval_correct,\n",
    "        images_placeholder,\n",
    "        labels_placeholder,\n",
    "        data_sets.validation)\n",
    "print('Test Data Eval:')\n",
    "do_eval(sess,\n",
    "        eval_correct,\n",
    "        images_placeholder,\n",
    "        labels_placeholder,\n",
    "        data_sets.test)\n",
    "```\n",
    "\n",
    "> Note that more complicated usage would usually sequester the `data_sets.test`\n",
    "> to only be checked after significant amounts of hyperparameter tuning.  For\n",
    "> the sake of a simple little MNIST problem, however, we evaluate against all of\n",
    "> the data.\n",
    "\n",
    "### Build the Eval Graph\n",
    "\n",
    "Before entering the training loop, the Eval op should have been built\n",
    "by calling the `evaluation()` function from `mnist.py` with the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits/labels parameters as the `loss()` function.\n",
    "\n",
    "```python\n",
    "eval_correct = mnist.evaluation(logits, labels_placeholder)\n",
    "```\n",
    "\n",
    "The `evaluation()` function simply generates a @{tf.nn.in_top_k}\n",
    "op that can automatically score each model output as correct if the true label\n",
    "can be found in the K most-likely predictions.  In this case, we set the value\n",
    "of K to 1 to only consider a prediction correct if it is for the true label.\n",
    "\n",
    "```python\n",
    "eval_correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "```\n",
    "\n",
    "### Eval Output\n",
    "\n",
    "One can then create a loop for filling a `feed_dict` and calling `sess.run()`\n",
    "against the `eval_correct` op to evaluate the model on the given dataset.\n",
    "\n",
    "```python\n",
    "for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "The `true_count` variable simply accumulates all of the predictions that the\n",
    "`in_top_k` op has determined to be correct.  From there, the precision may be\n",
    "calculated from simply dividing by the total number of examples.\n",
    "\n",
    "```python\n",
    "precision = true_count / num_examples\n",
    "print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "      (num_examples, true_count, precision))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
